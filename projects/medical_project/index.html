<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Outcome Prediction for Patients with Oropharyngeal Cancer | Chryssa M. Nampouri </title> <meta name="author" content="Chryssa M. Nampouri"> <meta name="description" content="MSc Thesis â€”Contrastive Self-supervised Learning for Outcome Prediction of Patients with Oropharyngeal Cancer. &lt;br&gt;"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://chryssanab.github.io/projects/medical_project/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Chryssa</span> M. Nampouri </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">ğŸ‘‹ğŸ¼ About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">ğŸ’¼ Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">ğŸ“‚ GitHub </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CM_Nampouri-CV.pdf">ğŸ“ Curriculum Vitae </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Outcome Prediction for Patients with Oropharyngeal Cancer</h1> <p class="post-description">MSc Thesis â€”Contrastive Self-supervised Learning for Outcome Prediction of Patients with Oropharyngeal Cancer. <br></p> </header> <article> <p style="font-size:22px"><b>Abstract</b></p> <hr> <p>With the emergence of new cancer subtypes and treatment options, there is a growing need for personalized treatment in patients with oropharyngeal squamous cell carcinoma (OPSCC). Developing robust outcome prediction models capable of identifying low and high-risk patients prior to treatment is a non-trivial task that may ultimately assist in stratifying patients for intensified or de-escalated treatment strategies, most suitable for them, without compromising their survival.</p> <p>Deep learning methods have demonstrated remarkable potential for predicting prognostic outcomes in head and neck cancers. The standard approach entails fully-supervised learning on volumetric medical images. However, annotating 3D medical images is an immensely time-consuming and expensive process, and in some cases, it may be even infeasible due to stringent privacy regulations. Consequently, the availability of labeled data in this domain is often limited, rendering the training process extremely challenging.</p> <p>Inspired by recent advances in self-supervised learning, this study delves into various <em>contrastive learning </em> frameworks for learning visual representations from medical images without relying on manual annotations. Throughout this endeavor, we also explore a diverse range of medical imaging modalities as input to determine the optimal configuration. Additionally, we conduct comparisons among different architectural choices, including <em>convolutional neural networks</em> and <em>vision transformers</em>. Furthermore, we investigate the extraction of features from multiple intermediate layers of these architectures to gain insights into the contribution of lower-level representations to the predictive performance of the models.</p> <p>The ultimate goal is to improve long-term survival rates by accurately identifying potential high-risk patients prior to treatment based solely on their diagnostic imaging tests. To this end, two datasets sourced from the publicly accessible <a href="https://www.cancerimagingarchive.net/" rel="external nofollow noopener" target="_blank">TCIA (The Cancer Imaging Archive)</a>, namely the <a href="https://wiki.cancerimagingarchive.net/display/Public/Head-Neck-PET-CT" rel="external nofollow noopener" target="_blank">Head-Neck-PET-CT</a> and <a href="https://wiki.cancerimagingarchive.net/display/Public/HNSCC" rel="external nofollow noopener" target="_blank">HNSCC</a> collections, are employed for pre-training the models. Subsequently, the <a href="https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=33948764" rel="external nofollow noopener" target="_blank">OPC-Radiomics</a> dataset from the same repository is utilized for fine-tuning. Finally, we assess the generalization ability of our models on an independent external set of 400 OPSCC patients provided by the <a href="https://umcgresearch.org/" rel="external nofollow noopener" target="_blank">University Medical Center Groningen, the Netherlands (UMCG)</a>. <br></p> <p>ğŸ“ˆ <u><b>Our best model achieves a <b>15% increase</b> in accuracy compared to the stateâ€‘ofâ€‘theâ€‘art methods.</b></u></p> <p><br></p> <p style="font-size:22px"><b>Research Questions</b></p> <hr> <p>We framed our investigation on contrastive representation learning for medical image analysis in terms of the following research questions:</p> <ul> <p> RQ1. Â  Â  <em> What is the optimal contrastive learning protocol? </em> </p> <p> RQ2. Â  Â  <em> What is the optimal medical imaging modality? </em> </p> <p> RQ3. Â  Â  <em> What is the optimal encoder for learning meaningful representations? </em> </p> <p>RQ4. Â  Â  <em> Does multi-level feature extraction empower contrastive representation learning? </em> </p> <p>RQ5. Â  Â  <em> Does ensemble learning boost performance over individual models? </em> </p> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2024 Chryssa M. Nampouri. Last updated: March 25, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>